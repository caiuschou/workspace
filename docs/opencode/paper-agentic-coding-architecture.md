# 基于 Agent 循环与工具调用的自动编程架构

**与实现语言无关的技术说明**

---

## 摘要

本文描述一种使大语言模型（LLM）能够安全、可控地修改本地代码库的架构。该架构由三部分组成：（1）**Agent 循环**——在单次用户请求内，反复将对话历史与工具列表送入 LLM，根据模型输出的文本或工具调用执行本地工具，并将工具结果作为新消息追加到历史后再次调用 LLM，直到模型决定结束；（2）**LLM 提供方抽象**——统一多厂商 API 的流式响应与工具调用（Tool Use）语义；（3）**一组面向文件编辑的工具**——包括单处替换/新建/删除（edit）、多文件统一补丁（patch）、整文件覆写（write），以及读与执行类工具（如文件浏览、搜索、执行 shell）。写盘前通过权限服务请求用户授权，并通过差异计算、可选的文件变更历史与语言服务器协议（LSP）诊断提升正确性与可追溯性。系统提示词约束模型采用「先读再改、绝对路径、最小改动、不自动提交」等策略。本文以抽象组件、数据流与协议描述为主，不依赖任何具体编程语言或代码库。

**关键词**：Agent 循环；Planner；ReAct；Plan-and-Execute；Tool Use；自动编程；LLM；任务分解；权限；diff；LSP

---

## 1. 引言

让 LLM 直接修改用户工作区中的代码，需要解决三类问题：**决策与执行的一体化**（模型何时调用何种工具、如何根据结果继续）、**与多种 LLM API 的对接**（流式输出、函数/工具调用格式不一）、**对文件系统修改的安全与可控**（避免误改、可审计、可回滚）。本文给出的架构通过一个固定的 **Agent 循环** 将「模型推理」与「工具执行」串联起来，通过 **Provider 抽象** 屏蔽各厂商 API 差异，通过 **edit / patch / write 三种编辑工具** 与 **权限服务、差异计算、历史与 LSP** 的配合，在实现层面满足上述需求。以下各节以与编程语言无关的方式定义组件、消息形态与流程，便于在不同语言与运行时中复现。

---

## 2. 架构总览

### 2.1 组件层次

- **Agent**：维护会话标识、消息历史与工具注册表；对外提供「运行一次用户请求」的入口；内部在单次运行中反复执行「把当前消息历史与工具描述发给 LLM → 接收流式输出与工具调用 → 在本地顺序执行被调用的工具 → 将助手消息与工具结果追加为两条新消息 → 用更新后的历史再次调用 LLM」，直到模型返回的结束原因为「结束回合」或发生取消/错误/权限拒绝。
- **Provider**：对 LLM 后端的抽象。至少支持：在给定消息序列与工具定义下，发起一次请求并返回**流式事件**（如文本增量、思考增量、工具调用开始/参数增量/结束、请求完成、错误）；在「请求完成」事件中提供完整工具调用列表与结束原因。不同厂商（如 OpenAI、Anthropic、Gemini 等）的 API 由各自适配器实现同一 Provider 接口，并将统一格式的工具定义转换为该 API 的 function/tool schema。
- **Tools**：可被模型按名称调用的能力单元。每个工具具有：名称、自然语言描述、参数模式（名称、类型、是否必填）；执行时接收调用标识、工具名与参数字符串（通常为 JSON），返回结果内容、可选元数据与是否出错的标志。与「写代码」直接相关的是三类文件编辑工具（见第 6 节），以及读与执行类工具（如列出目录、读取文件、搜索、执行 shell 等）。

### 2.2 数据流（单次用户请求）

1. 用户输入（文本及可选附件）与当前会话的历史消息合并，得到**当前消息历史**。
2. Agent 将**当前消息历史**与**工具定义列表**交给 Provider；Provider 向 LLM 发起请求并返回**事件流**。
3. Agent 消费事件流：累积助手消息的文本与思考内容；收集本轮的**工具调用列表**（每项含调用标识、工具名、输入参数）；在「请求完成」事件中得到**结束原因**。
4. 若结束原因为「工具调用」且存在工具调用列表：Agent 在本地按顺序执行每个工具调用，得到一组**工具结果**（每项含对应调用标识、结果内容、元数据、是否错误）；将**本轮的助手消息**与**一条由所有工具结果组成的工具消息**追加到消息历史；回到步骤 2。
5. 若结束原因为「结束回合」或其它（如达到最大 token、取消、错误、权限拒绝）：本轮结束，将最终的助手消息返回给调用方。

因此，「能写代码」在架构上等价于：**LLM 的 Tool Use 能力 + 一组可安全落盘的文件编辑工具 + 上述 Agent 循环**。

---

## 3. Agent 循环的细化

### 3.1 会话与上下文

- 每次「运行」绑定到一个**会话标识**。Agent 根据该标识加载该会话的**历史消息**（可能经过摘要截断：若会话存在「摘要消息」，则仅保留该消息及之后的消息，并将该消息视为一条用户消息作为上下文起点）。
- 用户本轮输入被持久化为一条**用户消息**（角色为用户；内容可包含文本与附件）。**当前消息历史** = 已加载的历史消息 + 该用户消息。

### 3.2 单轮「调用 LLM + 处理事件 + 执行工具」

- **调用 LLM**：Agent 以当前消息历史与工具注册表为输入，调用 Provider 的**流式接口**。Provider 将工具注册表转换为后端 API 所需的 schema，随请求发送；返回一个**事件流**。
- **处理流事件**：对每个事件按类型处理：
  - **思考增量**：追加到当前助手消息的「思考」内容并持久化。
  - **文本增量**：追加到当前助手消息的「文本」内容并持久化。
  - **工具调用开始**：为当前助手消息添加一项工具调用（标识、名称、初始参数）。
  - **工具调用结束**：标记该工具调用的参数已完整。
  - **请求完成**：用服务端返回的完整工具调用列表与结束原因更新当前助手消息并持久化；记录用量（若接口提供）。
  - **错误**：若为取消则按取消流程结束；否则向上层返回错误。
- **执行工具**：若当前助手消息包含工具调用列表，则按顺序对每一项：根据**工具名**在注册表中查找工具实现；若未找到则生成「工具未找到」的结果；若找到则传入调用标识、工具名与参数执行；若执行返回「权限拒绝」，则中止后续执行、将当前轮标记为「权限拒绝」并结束循环；否则将执行结果（调用标识、内容、元数据、是否错误）加入结果列表。最后，将**所有工具结果**持久化为一条**工具消息**（角色为工具），与当前助手消息一起作为本轮的输出。
- **决定是否继续**：若结束原因为「工具调用」且已成功生成工具结果消息，则将**当前助手消息**与**该工具消息**追加到当前消息历史，并开始下一轮（再次调用 LLM）；否则结束循环并返回当前助手消息。

### 3.3 取消与并发

- Agent 为每个会话的「当前运行」维护可取消的上下文；外部可请求取消，此时正在进行的 LLM 请求与未执行的工具调用应尽快终止，并将当前助手消息标记为「已取消」后结束。
- 同一会话在同一时刻至多允许一个运行中的请求（忙则拒绝新请求）。

---

## 4. LLM Provider 抽象

### 4.1 能力要求

- **流式接口**：输入为消息序列与工具定义；输出为**事件流**。事件至少包含：文本增量、思考增量（若模型支持）、工具调用开始/参数增量/结束、请求完成（含完整工具调用列表与结束原因）、错误。
- **非流式接口**（可选）：用于标题生成、会话摘要等辅助任务；输入同上，输出为完整响应（文本、工具调用、用量、结束原因）。
- **模型信息**：Provider 暴露当前使用的模型标识与能力（如是否支持附件、是否支持思考链等），供 Agent 做前置校验。

### 4.2 工具定义的转换

- 每个已注册工具向 Provider 提供：**名称**、**描述**（自然语言）、**参数模式**（参数名、类型、是否必填）。Provider 在发起请求前，将这些信息转换为目标 API 的 function/tool 定义（如 OpenAI 的 `tools`、Anthropic 的 `tools` 等）；模型返回的 tool_calls 由 Provider 解析为统一的**工具调用结构**（调用标识、工具名、参数字符串），通过「请求完成」事件或流中的工具调用事件交付给 Agent。

### 4.3 多后端

- 不同后端（OpenAI、Anthropic、Gemini、本地服务等）可实现同一 Provider 接口；Agent 仅依赖该接口，不关心具体后端。同一套工具定义可被不同后端的适配器转换成各自格式。

---

## 5. 消息与工具调用协议

### 5.1 消息角色

- **用户（user）**：用户输入，可包含文本与附件（如文件内容、图片）。
- **助手（assistant）**：模型输出，可包含文本、思考（若支持）、以及若干**工具调用**；消息末尾可有**结束标记**（见下）。
- **系统（system）**：系统提示词，通常由 Provider 在请求时注入，不单独持久化为会话内消息。
- **工具（tool）**：单条消息可包含多个**工具结果**，与上一条助手消息中的工具调用一一对应（通过调用标识关联）。

### 5.2 内容部件

- 一条消息由若干**内容部件**组成。典型类型：纯文本、思考内容、图片、二进制附件、**工具调用**（标识、名称、参数、是否已结束）、**工具结果**（调用标识、名称、内容、元数据、是否错误）、**结束标记**（结束原因、时间戳）。
- **结束原因** 取值包括：结束回合、达到最大 token、**工具调用**（表示模型希望执行工具，需继续循环）、已取消、错误、权限拒绝等。当且仅当结束原因为「工具调用」且存在工具结果时，Agent 继续下一轮。

### 5.3 工具调用与结果的对应

- 助手消息中可包含多个工具调用；执行顺序与消息内顺序一致。每条工具结果通过**调用标识**与某一条工具调用关联。工具消息按相同顺序列出所有结果，Agent 将整条工具消息与上一条助手消息一起追加到历史，供下一轮 LLM 使用。

---

## 6. 文件编辑工具

以下三种工具是「能写代码」的核心；均假设在写盘前通过**权限服务**请求用户授权，若用户拒绝则返回「权限拒绝」，Agent 终止本轮并标记为权限拒绝。

### 6.1 Edit（单处替换 / 新建 / 删除）

- **语义**：对单个文件做**一次**基于字符串匹配的替换，或新建文件，或删除一段内容。
- **参数**：文件路径（建议绝对路径）、旧字符串、新字符串。
  - **新建**：旧字符串为空；新字符串为完整内容；要求目标路径下文件尚不存在。
  - **删除**：新字符串为空；旧字符串为要删除的片段；要求该片段在文件中**唯一出现**（见下）。
  - **替换**：旧字符串被新字符串替换一次；同样要求**唯一匹配**。
- **约束**：
  - **先读再改**：目标文件必须在本次会话内曾被「读」类工具（如 View）读过，且自上次读到当前时刻未被外部修改；否则拒绝操作并提示先读。
  - **唯一性**：旧字符串在文件中的出现次数必须为 1（通过首尾索引相等判定）；若出现多次则要求调用方提供更多上下文以缩小到唯一位置。
- **流程**：解析参数 → 若为新建则检查不存在并计算 diff（空→内容）；若为删除或替换则读文件、校验先读与唯一性、计算新内容与 diff → 请求权限 → 写盘 → 可选地更新文件变更历史、触发 LSP 诊断并将诊断信息附在返回内容后。

### 6.2 Patch（多文件统一补丁）

- **语义**：在一次调用中，对多个文件应用**一组**变更（增/改/删），格式为自定义的块式 patch 文本（如「开始补丁」「更新文件 path」「添加文件 path」「删除文件 path」「结束补丁」及类 unified diff 的上下文行与增删行）。
- **参数**：补丁文本。
- **约束**：
  - 所有被「更新」或「删除」的文件必须先被读过，且自上次读以来未修改；所有被「添加」的路径必须尚不存在。
  - 补丁解析可产生**模糊度**（上下文匹配不精确）；当模糊度超过设定阈值时拒绝应用并建议提供更精确的上下文。
- **流程**：从补丁文本中识别涉及的文件路径（需读取的、需新增的）→ 校验先读与未修改、新增路径不存在 → 加载当前文件内容 → 解析补丁为**变更计划**（每路径一个动作：增/改/删及旧/新内容）→ 对每个变更生成展示用 diff 并请求权限 → 按计划依次写盘或删文件 → 更新历史与 LSP、汇总变更统计与诊断后返回。

### 6.3 Write（整文件覆写）

- **语义**：将给定路径的文件内容整体替换为给定内容；若文件不存在则创建。
- **参数**：文件路径、内容。
- **约束**：若文件已存在，须满足「先读再改」且自上次读以来未修改；若当前内容与请求内容相同则可不写盘直接返回「无变更」。
- **流程**：校验 → 计算 diff（旧内容→新内容）→ 请求权限 → 写盘 → 历史与 LSP、返回带 diff 统计的元数据。

### 6.4 共同依赖

- **差异计算**：提供「生成可读 diff（旧内容、新内容、路径）→ 文本 diff + 增/删行数」；用于权限请求时的展示与工具返回的元数据。
- **权限服务**：写盘或删文件前，以会话、路径、工具名、动作、描述与 diff 等为参数请求授权；返回允许/拒绝。拒绝时工具返回「权限拒绝」，由 Agent 统一处理。
- **文件变更历史**（可选）：按会话与路径记录每次写盘前后的内容版本，便于追溯与回滚。
- **LSP / 诊断**（可选）：写盘后可等待语言服务器诊断，并将诊断结果（错误、警告）附加到工具返回内容中，供模型在下一轮中修正。

---

## 7. 差异与补丁

### 7.1 差异生成

- 给定两个字符串（旧内容、新内容）与可选的文件路径，生成人类可读的 diff 文本及增行数、删行数。用于：权限请求时的变更展示；编辑工具返回的元数据。

### 7.2 补丁解析与应用

- **解析**：输入为补丁文本与当前各路径下的文件内容（path → content）；输出为**补丁结构**（每路径对应一组「行级删除/插入」或「新增/删除文件」）及**模糊度**。若模糊度超过阈值则拒绝应用。
- **变更计划**：将补丁结构转换为对文件系统的**原子变更计划**（每路径一个动作：添加、更新、删除，及对应的旧内容与新内容）。
- **应用**：按变更计划依次执行「写文件」或「删文件」；写文件时若目录不存在则先创建。可由调用方注入具体的写/删实现（如解析相对路径为绝对路径、创建父目录等）。

### 7.3 从补丁文本识别路径

- 提供从补丁文本中提取「需要读取当前内容的路径」与「将要新增的路径」的能力，供 Patch 工具做「先读再改」与「新增路径不存在」的校验。

---

## 8. 系统提示词与行为约束

### 8.1 角色与目标

- 系统提示词将助手定义为「面向终端的、可修改代码的编程助手」，要求**在问题解决前持续执行**，仅在确信任务完成或无法继续时结束回合；不要猜测文件内容或结构，应使用读类工具获取信息。

### 8.2 编辑规范

- **先读再改**：需要修改前先用读/浏览工具查看文件与路径，再使用 edit / patch / write。
- **路径**：使用绝对路径（或约定由系统将相对路径解析为基于工作目录的绝对路径）；一次 edit 只改一处；多处修改用多次 edit 或一次 patch。
- **自检**：改完后可用版本控制状态或 diff 自检；不自动提交除非用户明确要求。
- **风格**：回复简洁，少前言后语，适合终端展示；无依赖的多个工具调用可同批发出以节省轮次。

### 8.3 环境与 LSP

- 系统提示词可动态注入**环境信息**（工作目录、是否在版本库中、平台、日期、项目根目录列表等），以及若启用 LSP 则说明「工具返回中可能包含诊断，请根据诊断修复问题」。不同 LLM 后端可使用略有差别的 base 提示词，但都强调读→改→查与最小改动。

---

## 9. Agent 与 Planner：架构选择与学术脉络

本节着重区分**反应式 Agent（ReAct 式）**与**先规划再执行（Plan-and-Execute）**两类架构，并对照学术界对 LLM-Agent 规划能力的分类，说明本架构的定位与取舍。

### 9.1 本架构中的 Agent：ReAct 式循环执行体

- 本架构中的 **Agent** 指第 2、3 节定义的**循环执行体**：每轮由 LLM 根据当前消息历史直接决定输出文本或发起工具调用，执行后将结果写回消息并继续下一轮，**不先显式生成多步计划**。
- 该模式对应 **ReAct**（Reasoning + Acting）范式：推理（Thought）与行动（Action）交替进行，每步决策都基于**当前环境反馈**（工具结果、历史消息），形成「观察 → 推理 → 行动 → 观察」的闭环。
- **特点**：规划隐含在模型的逐轮输出中，通过系统提示词约束「先读再改、最小改动」等策略，通过工具选择与参数体现「下一步做什么」；无独立规划节点，无显式步骤列表或 DAG。

### 9.2 Plan-and-Execute（Planner）架构

- **Plan-and-Execute** 将**规划**与**执行**分离：先由「规划」模块（常为 LLM）产出显式多步计划（如步骤列表、子目标序列或 DAG），再由「执行」模块按计划逐步调用工具或子模型；子任务可并行或交给更小/专用模型。
- **典型优势**（据业界与综述）：多步工作流可减少每步都调用大模型的次数，有利于**成本与延迟**；前置显式推理可提升**复杂多步任务**的完成率；计划在执行前确定，对**间接提示注入**等攻击在控制流上更易约束。
- **典型劣势**：计划一旦错误或环境变化，需额外机制（如重规划、Reflection）修正；对需**实时适应**或反馈密集的任务，先规划再执行可能不够灵活。

### 9.3 ReAct 与 Plan-and-Execute 的对比与选用

| 维度 | ReAct（本架构） | Plan-and-Execute |
|------|-----------------|------------------|
| 规划形式 | 隐式，每轮一步，由 LLM 即时决策 | 显式，多步计划先产出再执行 |
| 环境反馈 | 每步都依赖上一步的工具/观察结果 | 执行阶段才密集依赖环境，规划阶段可少依赖 |
| 成本/延迟 | 每步一次 LLM 调用，步数多时成本高 | 规划一次、执行可复用或交给轻量模块，常更省 |
| 适应性 | 高，可随时根据结果改策略 | 依赖重规划或 Reflection，否则适应性较低 |
| 适用场景 | 探索性强、需实时纠错、步数不一定多的任务（如交互式改代码） | 步骤多、结构清晰、可预先拆解的任务 |

本架构选择 ReAct 式 Agent 的原因包括：自动编程场景中「读→改→查」强依赖**当前文件状态与 LSP 诊断**，每步决策需紧跟最新反馈；先验多步计划易因代码库复杂与用户意图模糊而失效；实现简单、无独立规划器与执行器状态机，便于多后端与多语言复现。

### 9.4 学术界对 LLM-Agent 规划的分类（综述对照）

Huang 等（2024）在 *Understanding the planning of LLM agents: A survey*（arXiv:2402.02716）中给出 LLM-Agent 规划的首个系统分类，将现有工作归纳为五类，可与本架构对照如下：

1. **Task Decomposition（任务分解）**  
   - **Decomposition-first**：先完整分解再逐子任务规划（如 HuggingGPT、Plan-and-Solve），对应「Plan-and-Execute」思路。  
   - **Interleaved decomposition**：分解与执行交错，每步只展开少量子目标并立即执行（如 ReAct、CoT），对应**本架构**：无显式分解步骤，但「每一步选哪个工具、改哪处」等价于即时子目标选择。

2. **Plan Selection（多计划选择）**  
   生成多条候选计划并用搜索/启发式选优（如 Tree-of-Thought、LLM-MCTS）。本架构**不采用**多计划生成与搜索，以单轨迹、流式决策为主，以控制复杂度和延迟。

3. **External Module（外部规划模块）**  
   LLM 负责形式化任务，外部符号/神经规划器负责生成计划（如 LLM+P/PDDL、SwiftSage）。本架构**不引入**外部规划器，规划完全由 LLM 与工具描述、提示词承担。

4. **Reflection and Refinement（反思与精炼）**  
   根据执行结果反思并修正计划（如 Reflexion、Self-Refine、CRITIC）。本架构**未**内置独立 Reflection 模块，但「工具结果写回历史 → 下一轮 LLM 根据结果继续」在行为上允许模型根据 LSP 诊断、权限拒绝等反馈做**隐式反思与下一步修正**；若需更强纠错，可在上层增加反思/重试策略。

5. **Memory-augmented Planning（记忆增强规划）**  
   从外部记忆（RAG、向量库、知识图谱等）检索经验以辅助规划。本架构依赖**会话内消息历史**（及可选的摘要截断）作为短期记忆；若产品需要长期记忆或跨会话经验，可在 Agent 之外增加记忆模块与检索接口，而不改变核心循环。

综上，本架构在学术分类上属于 **Task Decomposition 的 Interleaved 分支**（ReAct 式），不采用独立 Planner、多计划搜索或外部符号规划器；规划能力由**系统提示词 + 工具集 + 消息历史**共同承担，与「Agent 循环 + Provider 抽象 + 编辑工具」一起构成「能安全写代码」的最小闭环。

### 9.5 Planner 存在的理由：弥补 Agent 的不足

ReAct 式 Agent 的局限，正是 Plan-and-Execute 中 **Planner** 所要弥补的：

| Agent（ReAct）的不足 | Planner 的弥补方式 |
|----------------------|--------------------|
| **成本与延迟**：每步一次 LLM 调用，多步任务总调用次数多，成本与延迟线性增长。 | 规划阶段集中推理，产出多步计划；执行阶段可逐步执行且不必每步都调大模型，或由轻量执行器/规则执行，总调用次数与延迟可显著降低。 |
| **轨迹次优**：仅「当前一步」决策，缺乏全局视角，易出现重复尝试、绕路或遗漏依赖顺序。 | 先显式分解任务、排定步骤或 DAG，再按计划执行，有利于整体步骤顺序与依赖关系更合理。 |
| **认知负荷**：长链任务下，模型需在上下文中同时记住目标与已做步骤，易遗忘或偏离总目标。 | 计划作为显式中间表示，将「要做什么」与「正在做哪一步」分离，减轻单轮推理的认知负荷。 |
| **控制流与安全**：执行时每步都重新由 LLM 决定，易受工具返回内容中的间接提示注入影响，控制流难以审计。 | 计划在执行前确定，执行阶段按既定步骤调用工具，控制流更可预测、便于审计与约束。 |

因此，**Planner 的存在**，是为了在「步骤多、结构相对清晰、可预先拆解」的任务上，弥补纯 Agent 在**成本、全局一致性、长链稳定性与控制流可控性**上的不足；而在「探索性强、反馈密集、结构难预先确定」的任务上，Agent 的即时反应与纠错能力更合适。

### 9.6 Agent 与 Planner 的融合可能

两者可以在同一系统中共存或融合，常见形态包括：

1. **分层融合（Hierarchical）**  
   **上层 Planner + 下层 Agent**：Planner 产出高层子目标或步骤列表；每一步由 ReAct 式 Agent 在「当前子目标 + 当前环境」下执行工具调用，直到该步完成或失败再回到 Planner 或进入下一步。这样既保留全局结构（Planner），又保留局部适应与纠错（Agent）。例如：先规划「1) 读需求 2) 改 A 文件 3) 改 B 文件 4) 跑测试」，每一步由 Agent 循环完成。

2. **轻量规划 + 反应执行（Lightweight Plan then React）**  
   在进入主循环前，先做**一次**轻量规划（如只生成 3–5 条高层步骤或检查点），不展开到具体工具调用；主循环仍是 ReAct：每轮根据当前步骤与工具结果决定下一步。计划作为「路标」注入系统提示或上下文，Agent 可偏离路标以应对反馈。这样在不引入独立执行器的前提下，为 Agent 提供粗粒度全局视图。

3. **按需重规划（RePlan on Failure）**  
   默认以纯 Agent 运行；当检测到失败、死循环或用户触发时，插入一次**规划阶段**：根据当前状态与目标重新生成计划，再继续以 Agent 按新计划（或更新后的步骤描述）执行。Reflexion、Self-Refine 等「反思后修正」可视为一种隐式重规划。

4. **统一循环中的「计划状态」**  
   同一 Agent 循环内，LLM 既可输出**工具调用**，也可输出**对当前计划的增删改**（如「将步骤 3 提前」「增加一步：先读 X」）；计划作为可变的内部状态随执行演进，执行器仍按「当前计划 + 当前消息」决定是否调用工具。这样规划与执行在同一个循环中交替进行，而非严格两阶段。

**与本架构的关系**：当前文档描述的是**纯 ReAct 式 Agent**（无独立 Planner），以实现简单、反馈紧密为先。若产品需要在部分场景下兼顾「多步结构」与「成本/控制流」，可在不改变核心循环与工具协议的前提下，在上层增加**可选**的 Planner（如 1 或 2），或为 Agent 增加「计划状态」与相应提示（如 4），从而在**同一套消息与工具协议**下实现 Agent 与 Planner 的融合或切换。

### 9.7 学术界融合方向的最新研究

近年来有多项工作显式地将「全局规划」与「反应式执行」结合，在长时程、多步任务上显著优于纯 ReAct，可作为融合形态的实证参考。

**研究总表**（Agent 与 Planner 融合方向，按 arXiv/会议年份大致排序）：

| 序号 | 工作 | arXiv / 会议 | 研究状态 | 核心架构 | 融合形态（对应 9.6） | 主要基准 / 结果 | 热度 / 应用备注 |
|------|------|----------------|----------|----------|------------------------|------------------|------------------|
| 1 | **ReAct** | arXiv:2210.03629, ICLR 2023 | 完成 | 推理与行动交替，每步 Thought→Action→Observation | 基线（纯 Agent，无显式 Planner） | HotpotQA, Fever, ALFWorld, WebShop | 引用 5000+，工业界广泛参照 |
| 2 | **AdaPlanner** | NeurIPS 2023 | 完成 | 计划内/计划外自适应精炼，成功计划少样本复用 | 按需重规划(3) + 记忆 | ALFWorld, MiniWoB++；更少样本优于基线 | 学术复现 |
| 3 | **Tree-Planner** | arXiv:2310.08582, ICLR 2024 | 完成 | 计划采样 → 动作树构造 → 接地决策 | 轻量规划+反应执行(2)，树式 | token −92.2%，纠错 −40.5% | Star 数十量级 |
| 4 | **CoAct** | arXiv:2406.13381 | 完成 | 全局规划 Agent + 局部执行 Agent，失败时重排轨迹 | 分层(1) + 按需重规划(3) | WebArena 长时程 | 开源 |
| 5 | **Plan-on-Graph (PoG)** | arXiv:2410.23875 | 完成 | 子目标分解，路径探索，Guidance/Memory/Reflection 自校正 | 按需重规划(3) + 记忆增强 | 知识图谱推理 | 学术 |
| 6 | **Plan-and-Act** | arXiv:2503.09572, ICML 2025 | 完成 | Planner 模型 + Executor 模型，合成数据训 Planner，动态重规划 | 分层(1) + 按需重规划(3) | WebArena-Lite 57.58%, WebVoyager 81.36% | 代码/基准可复现 |
| 7 | **Plan-over-Graph** | arXiv:2502.14563 | 完成 | 子任务 → 抽象任务图，图上规划与并行调度 | 分层(1) + 图式 + 并行 | 全局效率 | 学术 |
| 8 | **GoalAct** | arXiv:2504.16563, NCIIP 2025 Best Paper | 完成 | 持续更新的全局规划 + 分层技能执行（搜索/编码/写作等） | 轻量规划+反应执行(2)，计划可迭代 | LegalAgentBench +12.22% | Star 十余 |
| 9 | **Vote-Tree-Planner** | arXiv:2502.09749 | 完成 | Tree-Planner + 投票选关键路径/执行顺序 | 树式 + 多计划选择 | 成功率与稳定性 | 学术 |
| 10 | **HiPlan** | arXiv:2508.19076 | 完成 | 里程碑动作库（专家示范）+ 自适应全局–局部引导 | 轻量规划+反应执行(2)，里程碑式 | 长时程基准 | 学术 |
| 11 | **ReAcTree** | arXiv:2511.02424 | 完成 | Agent 节点 + 控制流节点(→/?/⇒)，情节记忆+工作记忆 | 分层(1) + 控制流(4) | WAH-NL 61% vs ReAct 31%, ALFRED | 树式代表 |
| 12 | **Beyond ReAct** | arXiv:2511.10037 | 完成 | DAG 全局规划，SFT+GRPO 两阶段训练 | 显式 DAG 规划+执行(1)，训练增强 | 规划与工具选择 | 学术 |
| 13 | **Reason-Plan-ReAct (RP-ReAct)** | arXiv:2512.03560 | 完成 | RPA（高层分解与结果分析）+ PEA（ReAct 执行），外部存储 | 分层(1) + 上下文/存储分离 | ToolQA 企业多域 | 学术 |
| 14 | **Task-Decoupled Planning (TDP)** | arXiv:2601.07577 | 完成 | Supervisor（DAG 子目标）+ Planner/Executor 作用域推理 | 分层(1) + 作用域隔离 | TravelPlanner, ScienceWorld, HotpotQA；token −82% | 学术 |

*说明：**研究状态** 中「完成」表示本表该项已撰写专门章节（见 9.7.2–9.7.15）；其余表示发表与可复现情况（已发表=会议/期刊录用，预印本=仅 arXiv，开源=代码公开，工业参照=被工业产品/框架广泛参照）。融合形态栏中 (1)(2)(3)(4) 分别对应 9.6 的「分层融合」「轻量规划+反应执行」「按需重规划」「统一循环中的计划状态」。*

以下为各工作的展开说明：

- **ReAcTree**（arXiv:2511.02424）。在 ReAct 式 Agent 之上引入**动态构造的 Agent 树**：Agent 节点负责推理与行动并可继续分解子目标，**控制流节点**负责序列（→）、回退（?）、并行（⇒）等执行策略（类行为树）。并引入情节记忆（目标/子目标级示例）与工作记忆（环境观测）。在 WAH-NL 上 Qwen 2.5 72B 达 61% 目标成功率，约为纯 ReAct（31%）的近两倍；在 ALFRED 上也有稳定提升。对应融合形态：**分层 + 控制流**（9.6 中 1、4 的混合）。

- **GoalAct**（arXiv:2504.16563，NCIIP 2025 Best Paper）。**持续更新的全局规划** + **分层执行**（高层技能：搜索、编码、写作等）。与「先定死计划再执行」不同，其全局计划随执行反馈更新，兼顾高层引导与局部适应。在 LegalAgentBench 上相对基线平均提升 12.22%。对应融合形态：**轻量规划 + 反应执行**（9.6 中 2）的强化版，且计划可迭代更新。

- **Plan-and-Act**（arXiv:2503.09572，ICML 2025）。显式分离 **Planner 模型**（将用户目标分解为高层步骤）与 **Executor 模型**（将步骤转为具体环境动作），并用合成数据标注轨迹以训练 Planner；支持执行反馈后的**动态重规划**。在 WebArena-Lite 57.58%、WebVoyager 81.36%。对应融合形态：**分层 Planner + Executor**（9.6 中 1）+ **按需重规划**（9.6 中 3）。

- **CoAct**（arXiv:2406.13381）。**全局规划 Agent**（宏观计划与子任务描述）+ **局部执行 Agent**（多级任务下的具体执行）；遇失败时可**重新编排执行轨迹**而非固守初始计划。在 WebArena 长时程网页任务上优于基线。对应融合形态：**分层融合**（9.6 中 1）+ **按需重规划**（9.6 中 3）。

- **Plan-on-Graph（PoG）**（arXiv:2410.23875）。在知识图谱上做**自校正的自适应规划**：将问题分解为子目标，反复探索推理路径、更新记忆，并**反思是否需自我修正**。Guidance、Memory、Reflection 三机制协同。对应融合形态：**按需重规划** + 记忆增强（与 9.4 中 Memory-augmented Planning 结合）。

**其他类似工作（展开）**：

- **Reason-Plan-ReAct（RP-ReAct）**（arXiv:2512.03560）。将**战略规划**与**执行**解耦：**Reasoner-Planner Agent（RPA）** 负责高层任务分解、计划生成与执行结果分析（维护信念状态、生成抽象子问题以指导执行）；**Proxy-Execution Agent（PEA）** 将抽象子步骤转为具体工具调用，采用 ReAct 的 Thought→Action→Observation 循环。针对单 Agent 长链轨迹不稳定与上下文易溢出，引入**外部存储**管理大工具输出，按需加载而非全部塞入上下文。在 ToolQA 等企业多域任务上优于单 Agent，且在不同模型规模上更稳。对应融合形态：**分层 Planner + ReAct 执行**（9.6 中 1），并强调上下文与存储分离。

- **Task-Decoupled Planning（TDP）**（arXiv:2601.07577）。解决「纠缠上下文」：传统做法在单一长链中同时维护多子任务历史，易导致错误传播与认知负荷过高。TDP 引入 **Supervisor** 将复杂任务分解为带前驱约束的 **DAG 子目标**；**Planner** 与 **Executor** 仅在**当前活跃子任务**的作用域内推理与执行，不携带全任务历史。这样错误被限制在子任务内、可在局部重规划或修正，而不影响整体 DAG。在 TravelPlanner、ScienceWorld、HotpotQA 上 token 消耗较基线减少约 **82%**，鲁棒性提升。对应融合形态：**分层 + 作用域规划**（9.6 中 1），且规划与执行均按 DAG 子目标作用域隔离。

- **Tree-Planner**（arXiv:2310.08582）。将规划拆为三阶段以降低每步都调 LLM 的成本：**计划采样**（一次性或少量调用生成多步计划）、**动作树构造**（将计划展开为动作树）、**接地决策**（在树上做闭环决策，结合实时环境反馈选择下一步）。这样大部分推理集中在「计划采样」与「树构造」，执行阶段以「接地决策」为主，显著减少总 token（约 **−92.2%**）与纠错次数。对应融合形态：**轻量规划 + 反应执行**（9.6 中 2）的树式实现，强调效率。

- **Vote-Tree-Planner**（arXiv:2502.09749）。在 Tree-Planner 的**动作树**上增加**投票**机制：在真正执行前，对关键路径或分支用多个 LLM 调用（或多次采样）做评估与投票，选出更可靠的执行顺序或分支，再沿该路径执行。在保持或提升成功率的同时，通过减少无效分支的展开进一步控制 LLM 调用次数。对应融合形态：在 9.6 的**多计划选择**（Plan Selection）思路下，与树式规划结合，属于「树 + 投票」的混合。

- **HiPlan**（arXiv:2508.19076）。针对长时程任务中模型易迷失全局目标的问题，用**专家示范**离线构建**里程碑动作库**（milestone action guides）；执行时提供**自适应全局–局部引导**：高层给出当前阶段应达成的里程碑，局部给出逐步提示（step-wise hints），且可根据当前观测动态选择与当前状态最匹配的里程碑与提示。这样既保留全局结构，又允许执行阶段根据反馈微调。对应融合形态：**轻量规划 + 反应执行**（9.6 中 2），且规划侧以「里程碑 + 提示」的形式注入，而非显式步骤列表。

- **AdaPlanner**（NeurIPS 2023）。在**闭环**中根据环境反馈对计划做**自适应精炼**：除常规「按计划执行」外，支持**计划内精炼**（在计划内某步根据反馈调整后续步骤）与**计划外精炼**（发现计划不可行时跳出计划、重新规划再继续）。并用**成功计划**作为少样本示例，减少对大量任务示范的依赖。在 ALFWorld、MiniWoB++ 上以**更少样本**（约 2× 与 600× 更少）达到优于基线的表现。对应融合形态：**按需重规划**（9.6 中 3）+ 记忆/少样本复用（与 9.4 中 Memory 结合）。

- **Beyond ReAct**（arXiv:2511.10037）。以 **Planner 为中心**、用 **DAG 全局规划** 替代 ReAct 的逐步决策：先对复杂查询生成全局 DAG 计划（多步、多分支、多工具协同），再按 DAG 执行，从而摆脱 ReAct 的局部优化陷阱与工具协调不足。通过 **SFT + GRPO（Group Relative Policy Optimization）** 两阶段训练，提升模型的规划意识与工具选择准确性。对应融合形态：**显式 DAG 规划 + 执行**（9.6 中 1 的图式版本），并通过训练增强规划与工具使用能力。

- **Plan-over-Graph**（arXiv:2502.14563）。将任务分解为子任务后，构建**抽象任务图**（节点为子任务或抽象动作，边为依赖或顺序）；在图上做规划与调度，**自然支持并行**：无依赖的子任务可并行执行，从而在保证正确性的前提下提升全局效率。对应融合形态：**分层 + 图式规划**（9.6 中 1）+ 并行执行（与 9.6 中 1 的「子任务可并行」一致）。

上述工作的共同点：在保留**动作–观察反馈环**的前提下，引入显式全局规划（树/图/步骤列表或持续更新的计划），从而在长时程、多步任务上缓解 ReAct 的局部优化与轨迹次优问题；与本架构的融合思路（9.6）一致，可在不改变核心消息与工具协议的前提下，将其中一种形态作为可选上层扩展。

#### 热度与应用程度（截至检索时）

**学术热度**  
- **ReAct**（Yao et al., 2022）为奠基性工作：据 Semantic Scholar 等统计，引用量已达 **5000+**，被后续工具调用、Agent 基准与工业实现广泛参照。  
- **融合类工作**（ReAcTree、GoalAct、Plan-and-Act、CoAct、TDP、Tree-Planner、AdaPlanner 等）多为 **2023–2025 年** 顶会/预印（ICLR、NeurIPS、ICML、NCIIP 等），引用尚在积累；部分有开源代码：GoalAct、Tree-Planner、CoAct、AdaPlanner 等已在 GitHub 开源（Star 量多在数十量级，反映学术复现与跟进为主，尚未形成大规模生态）。  
- **综述类**（Huang et al. 规划综述、Masterman 架构综述、Li et al. 三范式综述）成为梳理 Agent/Planner 分类与选型的常用引用。

**工业应用**  
- **范式层面**：**ReAct 式**（每步推理+工具调用）与 **Plan-and-Execute**（先规划再执行）已被主流框架落地。例如 **LangChain / LangGraph** 提供官方 **Plan-and-Execute** 教程与实现（规划组件 + ReAct 式执行 Agent），并支持 Python/JavaScript；**NVIDIA AI Toolkit** 等将 ReAct Agent 作为可配置工作流提供。  
- **产品层面**：**Cursor 2.0** 等采用「Agent 优先」、多 Agent 并行与 Composer 模型做多文件与仓库级修改，整体仍偏向**反应式 + 工作流**，未明确对外暴露「独立 Planner 节点」。企业侧常见 **Manager–Specialist–Worker** 等分层 Agent 模式（高层分解、专家执行），与学术上的「全局规划 + 局部执行」思路一致，但多为自研或基于通用框架二次开发，与单篇论文难以一一对应。  
- **基准与上限**：学术 SOTA 在 WebArena、WorkBench、VitaBench 等真实/多工具场景下，成功率仍多处于 **约 30%–60%**；工业界在可控场景中通过提示、工作流与人工兜底达到可用体验，复杂长链、多工具、开放环境的「全自动 Agent」仍处于探索与试点阶段。

**小结**：ReAct 与 Plan-and-Execute 的**思路**已在学术界与工业界普及；**具体融合形态**（ReAcTree、GoalAct、TDP、Tree-Planner 等）以学术验证与框架级实现为主，尚未见大规模产品化报道。选型时可将「热度高、引用多」的 ReAct/Plan-and-Execute 作为基线，将融合工作视为在**长时程、多步、成本敏感**场景下的可选增强。

### 9.7.1 研究：分类、脉络与本架构对照

本节对 9.7 中的融合方向研究做专门归纳，便于读者按「研究问题—分类—方法—基准—选型」梳理脉络，并与本架构对照。

#### 9.7.1.1 研究定位与核心问题

- **为何要做 Agent–Planner 融合**：纯 ReAct 在长时程、多步任务上存在**成本与延迟**（每步一次 LLM）、**轨迹次优**（缺乏全局视角）、**认知负荷**（长链易遗忘目标）与**控制流可审计性**不足；Plan-and-Execute 的 Planner 可弥补上述不足，但需与反应式执行结合以应对环境变化与执行失败。
- **核心研究问题**：如何在保留**动作–观察反馈环**的前提下，引入显式或轻量级全局规划，从而在**不显著增加实现复杂度**的情况下提升长时程任务的成功率、降低 token 消耗并提高鲁棒性。

#### 9.7.1.2 按融合形态的分类（对应 9.6）

将 9.7 研究总表中的 14 项工作按 9.6 的四种融合形态归纳，便于对照选型：

| 融合形态（9.6） | 代表工作（表序号） | 要点 |
|-----------------|--------------------|------|
| **(1) 分层融合** | CoAct(4)、Plan-and-Act(6)、Plan-over-Graph(7)、ReAcTree(11)、Beyond ReAct(12)、RP-ReAct(13)、TDP(14) | 上层 Planner/Supervisor 产出计划或子目标，下层 Agent/Executor 按步或按子任务执行；部分支持按需重规划。 |
| **(2) 轻量规划+反应执行** | Tree-Planner(3)、GoalAct(8)、HiPlan(10) | 先做一次轻量规划（步骤列表、里程碑或动作树），主循环仍为 ReAct，计划作路标可偏离。 |
| **(3) 按需重规划** | AdaPlanner(2)、CoAct(4)、PoG(5)、Plan-and-Act(6) | 默认执行中遇失败/不可行时触发重规划或反思，再继续执行。 |
| **(4) 统一循环中的计划状态** | ReAcTree(11)（控制流节点） | 同一循环内既有工具调用也有对「当前计划」的增删改，计划随执行演进。 |

部分工作跨类（如 CoAct 同时属于 (1)(3)，ReAcTree 同时属于 (1)(4)）；树式、图式、DAG 等为**实现形态**，可落入 (1)(2)(4) 中某一类或组合。

#### 9.7.1.3 方法论与评估基准

- **常见基准**：WebArena / WebArena-Lite、WebVoyager、ALFWorld、MiniWoB++、HotpotQA、ScienceWorld、TravelPlanner、WAH-NL、ALFRED、LegalAgentBench、ToolQA、GAIA 等；侧重长时程、多步、多工具或真实环境。
- **常见指标**：任务成功率（或 Pass@1）、总 token 消耗、纠错/重试次数、与基线的相对提升（如 token −82%、−92.2%）；部分工作报告不同模型规模下的鲁棒性。
- **复现要点**：多数为学术复现；部分提供代码与基准（如 Plan-and-Act、Tree-Planner、CoAct、GoalAct、AdaPlanner）；工业参照以 ReAct 与 LangChain/LangGraph 的 Plan-and-Execute 为主。

#### 9.7.1.4 与本架构的对照与选型建议

- **本架构在分类中的位置**：属**纯 ReAct**（无显式 Planner），对应 9.4 的 **Task Decomposition — Interleaved**；规划由系统提示词 + 工具集 + 消息历史承担。
- **可选扩展**：在不改变核心消息与工具协议的前提下，可按场景选 9.6 中一种形态作为上层扩展——例如**长时程、步骤清晰**时采用分层(1)或轻量规划(2)，**成本敏感**时采用 Tree-Planner 式(2)或 TDP 式(1)以降低 token，**需强纠错**时引入按需重规划(3)。
- **选型建议**：以 ReAct/Plan-and-Execute 为基线理解；将 9.7 表中的融合工作视为在**长时程、多步、成本敏感**场景下的可选增强，按基准与指标选取与自身任务最接近的形态做复现或简化实现。

#### 9.7.1.5 最新进展（截至文档更新时）

- **ReAct&Plan 综述观点**：近期综述与博客将「ReAct 与 Plan 融合」归纳为趋势：结合**单步反应**与**全局多步规划**，以初始全局计划（常为 DAG）+ 动态重规划缓解纯 ReAct 的局部优化陷阱，并分离高层战略规划与低层执行，兼顾错误恢复（反应环）与前瞻（规划）。
- **JoyAgent-JDGenie**（arXiv:2510.00510，京东开源）：面向生产的**多 Agent 框架**，显式结合 **Plan-Execute**（Supervisor 生成高层计划、按步执行并周期性反思）与 **ReAct**（多单 Agent 在不确定下探索）；通过**后验投票**（3–5 个模型）聚合输出，并采用**分层记忆**（工作记忆、语义记忆、程序记忆）。在 GAIA 验证集 75.15%、测试集 65.12%，优于多款业界与开源基线。可作为「融合形态在通用智能体系统中的工程化」的参照，与本架构的「单 Agent 循环 + 可选 Planner 扩展」形成对比：前者偏多 Agent 与投票，后者偏单会话、单模型流式循环。

以上 9.7.1 与 9.7 总表及展开说明一起，构成对「学术界融合方向最新研究」的专门章节；后续可随新论文与基准持续增补表项与 9.7.1.5。

### 9.7.2 ReAct（表格项 #1）：范式、基准与本架构对应

本节对 9.7 研究总表**第 1 项 ReAct** 做专门研究性归纳，作为「纯 Agent、无显式 Planner」的基线参照，并与本架构对应。

#### 9.7.2.1 出处与定位

- **文献**：Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., & Cao, Y. *ReAct: Synergizing Reasoning and Acting in Language Models*. arXiv:2210.03629 (2022), ICLR 2023（Oral，约 top 5%）；v3 为 ICLR camera-ready。
- **作者单位**：论文署名含 Google Research 等；项目与代码见 react-lm.github.io 及 GitHub（如 ysymyth/ReAct）。
- **定位**：将 LLM 的**推理**（reasoning，如 chain-of-thought）与**行动**（acting，如调用 API、与环境交互）从「分别研究」转为**交织进行**的通用范式，是后续「工具调用 + Agent 循环」类工作的奠基性工作之一；在 9.7 表中作为**基线**（纯 Agent，无显式 Planner）。

#### 9.7.2.2 核心思想与协议

- **范式名称**：ReAct = **Re**asoning + **Act**ing；强调二者**协同**而非分离。
- **单步结构**：每步由模型生成**推理轨迹**（Thought）与**任务相关动作**（Action），执行后获得**观察**（Observation），再进入下一步，形成 **Thought → Action → Observation** 的循环。
- **推理轨迹的作用**：用于诱导、跟踪与更新动作计划，并处理异常；**动作**用于与外部源（知识库、环境）交互以获取额外信息，从而缓解纯 CoT 的幻觉与错误传播。
- **与 CoT、纯 Acting 的对比**：相较「仅推理」的 chain-of-thought，ReAct 通过与环境/API 交互获取 grounded 信息；相较「仅行动」的纯动作序列，ReAct 的推理轨迹提升可解释性、可信度与可诊断性。

#### 9.7.2.3 实验基准与主要结果

- **语言与知识类**  
  - **HotpotQA**（多跳问答）：通过与简单 Wikipedia API 交互，缓解 CoT 的幻觉与错误传播，生成更可解释的轨迹。  
  - **Fever**（事实验证）：同样借助外部检索/API，优于无推理或无动作的基线。
- **交互决策类**  
  - **ALFWorld**（具身文本决策）：仅用 1–2 个 in-context 示例，相较模仿学习与强化学习方法**绝对成功率提升约 34%**。  
  - **WebShop**（网页购物任务）：同上设置，**绝对成功率提升约 10%**。
- **效益**：论文强调在可解释性、可信度、可诊断性与可控性上优于「仅推理」或「仅行动」的方法。

#### 9.7.2.4 影响与后续

- **学术影响**：据 Semantic Scholar 等统计，引用量已达 **5000+**；后续工具调用、Agent 基准（如 WebArena、GAIA）、以及 9.7 表中多数融合工作均以 ReAct 为基线或参照。
- **工业应用**：LangChain / LangGraph、NVIDIA AI Toolkit 等将 ReAct 式 Agent 作为可配置范式提供；Cursor、Copilot 等产品中的「推理 + 工具调用」循环在思想上与 ReAct 一致。
- **局限与融合动机**：ReAct 为**隐式规划、每步依赖当前观察**，在长时程、多步任务上存在成本高、轨迹次优、认知负荷与控制流难审计等问题，故 9.6、9.7 中各类「Plan + ReAct」融合工作旨在在保留 Thought–Action–Observation 环的前提下引入显式或轻量规划以弥补上述不足。

#### 9.7.2.5 与本架构的对应关系

- **本架构中的 Agent 循环**（第 2、3 节）与 ReAct 的 **Thought → Action → Observation** 在**结构上一致**：每轮将当前消息历史与工具列表交给 LLM，模型输出文本（含推理/思考）与工具调用（Action），执行后工具结果作为 Observation 追加到历史，再进入下一轮；无独立 Planner，规划隐含在模型的逐轮输出与系统提示词中。
- **术语对应**：本架构的「助手消息（含思考与工具调用）」「工具消息（工具结果）」即 ReAct 的 Thought+Action 与 Observation 的具化；本架构的 **Provider 抽象**与**编辑类工具**是在 ReAct 范式下对「多后端 + 安全写代码」的工程化扩展。
- **结论**：本架构可视为 ReAct 范式在「自动编程、多后端、文件编辑与权限」场景下的**实例化**；9.7 表中序号 2–14 的融合工作均是在此 ReAct 基线之上的增强，选型时以 ReAct（#1）为共同参照。

### 9.7.3 AdaPlanner（表格项 #2）：自适应精炼与本架构对照

本节对 9.7 研究总表**第 2 项 AdaPlanner** 做专门研究性归纳，作为「按需重规划(3) + 记忆」形态的代表，并与本架构对照。

#### 9.7.3.1 出处与定位

- **文献**：Sun, H., Zhuang, Y., Kong, L., Dai, B., & Zhang, C. *AdaPlanner: Adaptive Planning from Feedback with Language Models*. arXiv:2305.16653 (2023), NeurIPS 2023。
- **作者单位**：Georgia Tech 等；代码见 GitHub（如 haotiansun14/AdaPlanner）。
- **定位**：针对「贪婪执行无规划」或「静态计划无法随环境反馈调整」导致随问题复杂度与规划视野增加而性能下降的问题，提出**闭环**自适应规划：LLM 根据环境反馈对自生成计划进行精炼，对应 9.6 的**按需重规划(3)**，并与成功计划的少样本复用（记忆）结合。

#### 9.7.3.2 核心思想与协议

- **闭环精炼**：Agent 先生成计划，执行过程中根据**环境反馈**对计划进行**自适应精炼**，而非固守初始静态计划。
- **计划内精炼（in-plan refinement）**：在按计划执行时，若某步的反馈表明后续步骤需调整，则在**计划内部**修改后续步骤再继续执行。
- **计划外精炼（out-of-plan refinement）**：当发现当前计划不可行或环境与预期不符时，**跳出当前计划**、重新规划后再继续执行。
- **Code-style 提示结构**：采用类代码结构的 LLM 提示，减少幻觉并便于在不同任务、环境与 Agent 能力下生成与精炼计划。
- **技能发现（skill discovery）**：将**成功计划**作为少样本示例复用，使 Agent 在**更少任务示范**下仍能进行规划与精炼（ALFWorld 约 2× 更少、MiniWoB++ 约 600× 更少）。

#### 9.7.3.3 实验基准与主要结果

- **ALFWorld**（具身文本决策）：相较当时 SOTA 基线**绝对成功率提升约 3.73%**，且使用约 **2× 更少**的样本（少样本设置）。
- **MiniWoB++**（网页交互基准）：相较当时 SOTA 基线**绝对成功率提升约 4.11%**，且使用约 **600× 更少**的样本。
- **要点**：在保持或提升性能的前提下，显著降低对任务示范数量的依赖，体现「按需重规划 + 成功计划记忆」在样本效率上的优势。

#### 9.7.3.4 影响与后续

- **学术影响**：NeurIPS 2023 录用；被 9.7 表中后续融合工作（如 CoAct、Plan-and-Act、PoG）在「按需重规划」思路上引用或对照；开源代码便于复现。
- **与 ReAct 的关系**：AdaPlanner 在「先有计划、再执行」的基础上增加**闭环精炼**，可视为在 ReAct 式单步决策之外引入显式计划与重规划；与本架构的纯 ReAct 相比，多了一层「计划生成 → 执行 → 反馈 → 计划内/外精炼」的闭环。
- **局限**：实验集中于 ALFWorld、MiniWoB++；在更开放或长时程基准（如 WebArena、GAIA）上的表现需参考后续工作（如 CoAct、Plan-and-Act）。

#### 9.7.3.5 与本架构的对应关系

- **本架构当前无显式「计划精炼」节点**：Agent 循环（第 2、3 节）每轮根据消息历史与工具结果决定下一步，行为上可依赖工具返回（如 LSP 诊断、权限拒绝）做**隐式**下一步调整，但无「计划内/计划外精炼」的显式协议。
- **可选扩展**：若需在自动编程场景中引入「先粗计划、再执行、遇反馈再精炼」，可在不改变核心消息与工具协议的前提下，在上层增加 AdaPlanner 式的**按需重规划**模块：当检测到失败、死循环或用户触发时，调用一次规划/精炼步骤，将更新后的步骤描述或子目标注入上下文，再继续以现有 Agent 循环执行（对应 9.6 中形态 (3)）。
- **结论**：AdaPlanner 作为 9.7 表#2、融合形态「按需重规划(3) + 记忆」的代表，为本架构在**长时程、需少样本或需显式纠错**场景下的可选增强提供参照；选型时若强调「计划可随反馈调整」与「少样本规划」，可优先参考 AdaPlanner 的闭环精炼与技能发现设计。

### 9.7.4 Tree-Planner（表格项 #3）：轻量规划+反应执行与本架构对照

本节对 9.7 研究总表**第 3 项 Tree-Planner** 做专门研究性归纳，作为「轻量规划+反应执行(2)，树式」形态的代表，并与本架构对照。

#### 9.7.4.1 出处与定位

- **文献**：Hu, M., Mu, Y., Yu, X., Ding, M., Wu, S., Shao, W., Chen, Q., Wang, B., Qiao, Y., & Luo, P. *Tree-Planner: Efficient Close-loop Task Planning with Large Language Models*. arXiv:2310.08582 (2023), ICLR 2024；v2 为 ICLR 出版版本。
- **作者单位**：论文署名含多所机构；项目见 tree-planner.github.io，代码见 GitHub（如 aaron617/tree-planner）。
- **定位**：针对「逐轮用 LLM 迭代生成动作」带来的**高 token 消耗**与**重复纠错**两大低效问题，将任务规划重构为三阶段（计划采样 → 动作树构造 → 接地决策），在保持闭环任务规划（根据实时观察调整计划）的前提下显著降 token、减纠错，对应 9.6 的**轻量规划+反应执行(2)**的树式实现。

#### 9.7.4.2 核心思想与协议

- **Close-loop task planning**：指在达成目标的过程中**生成技能序列（计划）**，并**根据实时观察**对计划进行适应；Tree-Planner 保持闭环，但将 LLM 调用从「每步都做完整推理」拆成「少量计划侧调用 + 多步轻量决策侧调用」。
- **三阶段**：  
  1. **Plan sampling（计划采样）**：执行前用 LLM 根据常识**采样一组**潜在计划（一次性或少量调用），而非执行中每步都重新规划。  
  2. **Action tree construction（动作树构造）**：将采样得到的多份计划**聚合**为统一的**动作树**（树节点对应动作或技能，分支对应不同计划路径）。  
  3. **Grounded deciding（接地决策）**：执行阶段 LLM 在动作树上做**自顶向下**的决策，每步结合**实时环境观测**选择下一动作或分支；大量 prompt 无需每步重复注入，从而降低 token 消耗；若某路径失败，可在树上**回溯**到其他分支再试，纠错更灵活。
- **与纯 ReAct 的对比**：ReAct 每步「Thought → Action → Observation」都伴随完整上下文与推理，token 线性增长；Tree-Planner 将「规划」前置于计划采样与树构造，执行阶段以「在树上选下一步」为主，单次调用更轻、重复内容更少，故 token 大幅下降（论文报告相较当时最佳模型 **−92.2%**），纠错次数下降 **−40.5%**（得益于树上回溯）。

#### 9.7.4.3 实验基准与主要结果

- **Token 消耗**：相较当时最佳模型（逐轮迭代生成动作的范式），**token 消耗减少约 92.2%**；主要来自将查询分解为「一次计划采样调用 + 多次接地决策调用」，使大量 prompt 不再每步重复消耗。
- **纠错**：通过在动作树上**按需回溯**，纠错过程更灵活，**纠错次数减少约 40.5%**。
- **性能**：在论文所用基准上达到当时 **SOTA** 水平，同时保持高效率；具体基准名称见原文（与闭环任务规划、技能序列相关）。
- **要点**：在保持或提升任务表现的前提下，显著降低 token 与纠错成本，适合**成本敏感、步数多**的闭环规划场景。

#### 9.7.4.4 影响与后续

- **学术影响**：ICLR 2024 录用（Poster）；被 9.7 表中后续工作引用（如 Vote-Tree-Planner 在 Tree-Planner 动作树上增加投票机制）；开源代码与项目页便于复现，Star 量在数十量级。
- **与 ReAct、AdaPlanner 的关系**：Tree-Planner 不放弃「执行时根据观察做决策」的闭环，但将「重规划/多步推理」前移到「计划采样 + 树构造」，执行阶段以轻量接地决策为主，与 ReAct 的每步重推理、AdaPlanner 的显式计划精炼形成互补；同属 9.6 中「轻量规划+反应执行(2)」的树式实现。
- **局限**：计划采样与树构造依赖任务与技能的可枚举/可聚合性；在极度开放、动作空间极大的场景下，树的规模与构造成本需额外考虑。

#### 9.7.4.5 与本架构的对应关系

- **本架构当前为纯 ReAct 式**：每轮完整消息历史与工具列表送入 LLM，无「计划采样 + 动作树」的前置阶段；token 消耗与步数近似线性。
- **可选扩展**：若需在自动编程场景中**显著降低 token 与纠错成本**（如多文件、多步骤的批量修改），可在不改变核心消息与工具协议的前提下，在上层增加 Tree-Planner 式的**轻量规划+树式执行**：先做一次或少量「计划采样」调用得到高层步骤或候选序列，构造成树或步骤列表后，再以现有 Agent 循环在「当前步骤/分支 + 工具结果」上做接地决策，必要时在树/列表上回溯（对应 9.6 中形态 (2)）。
- **结论**：Tree-Planner 作为 9.7 表#3、融合形态「轻量规划+反应执行(2)，树式」的代表，为本架构在**成本敏感、步数多、可枚举子步骤**场景下的可选增强提供参照；选型时若强调「降 token、减纠错」且任务可分解为可聚合的步骤/技能，可优先参考 Tree-Planner 的三阶段与动作树回溯设计。

### 9.7.5 CoAct（表格项 #4）：分层融合+按需重规划与本架构对照

本节对 9.7 研究总表**第 4 项 CoAct** 做专门研究性归纳，作为「分层(1) + 按需重规划(3)」形态的代表，并与本架构对照。

#### 9.7.5.1 出处与定位

- **文献**：Hou, X., Yang, M., Jiao, W., Wang, X., Tu, Z., & Zhao, W. X. *CoAct: A Global-Local Hierarchy for Autonomous Agent Collaboration*. arXiv:2406.13381 (2024)；9 页，4 图。
- **作者单位**：论文署名含多所机构；代码见 GitHub（如 xmhou2002/CoAct、dxhou/coact）。
- **定位**：针对现有 LLM 即便采用 CoT、ReAct 等策略仍难以胜任**复杂真实世界任务**的问题，将人类社会的**分层规划与协作模式**迁移到 LLM 系统，提出**全局规划 Agent + 局部执行 Agent** 的双层架构，并在遇失败时**重新编排执行轨迹**，对应 9.6 的**分层融合(1) + 按需重规划(3)**。

#### 9.7.5.2 核心思想与协议

- **双 Agent 层次**：  
  1. **Global Planning Agent（全局规划 Agent）**：理解问题范围、制定宏观计划，并向局部执行 Agent 提供**详细子任务描述**，作为全局计划的初始版本。  
  2. **Local Execution Agent（局部执行 Agent）**：在**多层级任务执行结构**内运作，专注于在全局计划下对**具体子任务**做细粒度执行与实现。
- **协作模式**：与人类社会的分层规划与协作模式对应——高层负责「做什么、拆成哪些子任务」，低层负责「怎么做、逐步执行」。
- **按需重规划**：当**面对失败**时，CoAct 可**重新编排过程轨迹**（re-arrange the process trajectory），而非固守初始计划，从而结合分层(1)与按需重规划(3)。

#### 9.7.5.3 实验基准与主要结果

- **WebArena**：真实网页环境基准，包含电商、社交论坛、软件开发、内容管理等领域的完整功能网站；用于评估**长时程网页任务**上的表现。
- **结果**：在 WebArena 上相较基线方法取得**更优性能**（superior performance），尤其在长时程网页任务上；遇失败时可重排轨迹以继续或纠错。
- **要点**：在真实、多步骤、多网站的开放网页场景下，验证「全局规划 + 局部执行 + 失败时重排」的有效性；代码开源便于复现。

#### 9.7.5.4 影响与后续

- **学术影响**：arXiv 2024；被 9.7 表中后续融合工作（如 Plan-and-Act、TDP）在「分层 + 重规划」思路上引用或对照；开源代码便于复现。
- **与 ReAct、AdaPlanner、Tree-Planner 的关系**：CoAct 显式引入**两层 Agent**（规划层 + 执行层），执行层可在多级任务结构下细粒度执行；遇失败时重排轨迹，与 AdaPlanner 的「计划外精炼」、Tree-Planner 的「树上回溯」形成互补；同属 9.6 中「分层(1) + 按需重规划(3)」的典型实现。
- **局限**：实验集中于 WebArena 长时程网页任务；在自动编程、多文件编辑等场景下的适配需参考本架构的 Agent 循环与工具集。

#### 9.7.5.5 与本架构的对应关系

- **本架构当前为单 Agent 循环**：无独立的「全局规划 Agent」与「局部执行 Agent」；规划隐含在系统提示词与消息历史中，执行即单循环内的工具调用。
- **可选扩展**：若需在自动编程场景中引入**分层规划 + 失败重排**（如多文件、多步骤的仓库级修改），可在不改变核心消息与工具协议的前提下，在上层增加 CoAct 式的**双层结构**：**全局规划 Agent** 产出高层计划与子任务描述（如「1) 读需求 2) 改 A 文件 3) 改 B 文件 4) 跑测试」），**局部执行 Agent** 采用现有 Agent 循环在「当前子任务 + 工具结果」下执行；遇失败或不可行时触发**轨迹重排**（重新规划或调整子任务顺序），再继续执行（对应 9.6 中形态 (1)(3)）。
- **结论**：CoAct 作为 9.7 表#4、融合形态「分层(1) + 按需重规划(3)」的代表，为本架构在**长时程、多步骤、真实环境（如网页或跨文件）且需失败恢复**场景下的可选增强提供参照；选型时若强调「全局计划 + 局部执行 + 遇败重排」，可优先参考 CoAct 的双 Agent 层次与轨迹重排设计。

### 9.7.6 Plan-on-Graph（PoG）（表格项 #5）：按需重规划+记忆增强与本架构对照

本节对 9.7 研究总表**第 5 项 Plan-on-Graph (PoG)** 做专门研究性归纳，作为「按需重规划(3) + 记忆增强」形态的代表，并与本架构对照。

- **出处与定位**：*Plan-on-Graph: Self-Correcting Adaptive Planning of Large Language Model on Knowledge Graphs*. arXiv:2410.23875 (2024), NeurIPS 2024。代码见 GitHub（如 liyichen-cly/PoG）。将 LLM 与知识图谱结合，在图上做**自校正的自适应规划**，对应 9.6 的**按需重规划(3) + 记忆增强**（与 9.4 Memory-augmented Planning 结合）。
- **核心思想与协议**：将问题分解为子目标，在知识图谱上反复**探索推理路径**、**更新记忆**，并**反思是否需自我修正**；Guidance、Memory、Reflection 三机制协同，循环直至得到答案。
- **实验基准与主要结果**：在多个真实世界知识图谱数据集上验证有效性与效率，优于现有 KG 增强 LLM 方法。
- **与本架构的对应关系**：本架构无显式「图上规划 + 反思自校正」；若自动编程场景需**知识库/图谱辅助规划**（如 API 文档、依赖图），可参考 PoG 的 Guidance/Memory/Reflection 在上层扩展。选型时若强调「知识增强 + 自校正规划」，可参考 PoG。

### 9.7.7 Plan-and-Act（表格项 #6）：分层+按需重规划与本架构对照

本节对 9.7 研究总表**第 6 项 Plan-and-Act** 做专门研究性归纳，作为「分层(1) + 按需重规划(3)」的典型实现，并与本架构对照。

- **出处与定位**：*Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks*. arXiv:2503.09572 (2025), ICML 2025。作者含 Lutfi Eren Erdogan, Nicholas Lee, Sehoon Kim 等。显式分离 **Planner 模型**与 **Executor 模型**，用合成数据标注轨迹以训练 Planner，支持**动态重规划**。
- **核心思想与协议**：Planner 将用户目标分解为高层步骤；Executor 将步骤转为具体环境动作；遇执行反馈可触发重规划。合成数据增强计划生成泛化。
- **实验基准与主要结果**：WebArena-Lite **57.58%**、WebVoyager（text-only）**81.36%**，达当时 SOTA；代码/基准可复现。
- **与本架构的对应关系**：本架构为单 Agent 循环；若需「可训练 Planner + 显式 Executor + 动态重规划」，可参考 Plan-and-Act 在上层增加双模型与合成数据流程。选型时若强调长时程网页任务与可训练规划能力，可优先参考 Plan-and-Act。

### 9.7.8 Plan-over-Graph（表格项 #7）：分层+图式+并行与本架构对照

本节对 9.7 研究总表**第 7 项 Plan-over-Graph** 做专门研究性归纳，作为「分层(1) + 图式 + 并行」形态的代表，并与本架构对照。

- **出处与定位**：arXiv:2502.14563 (2025)。将任务分解为子任务后，构建**抽象任务图**（节点为子任务或抽象动作，边为依赖或顺序），在图上做规划与调度。
- **核心思想与协议**：图上规划**自然支持并行**：无依赖的子任务可并行执行，在保证正确性的前提下提升全局效率。对应 9.6 中分层(1)的「子任务可并行」。
- **与本架构的对应关系**：本架构为单线程 Agent 循环；若需多子任务**并行执行**（如多文件独立修改），可在上层引入任务图与调度器，执行层仍用现有 Agent 循环。选型时若强调「DAG 子任务 + 并行」，可参考 Plan-over-Graph。

### 9.7.9 GoalAct（表格项 #8）：轻量规划+反应执行（计划可迭代）与本架构对照

本节对 9.7 研究总表**第 8 项 GoalAct** 做专门研究性归纳，作为「轻量规划+反应执行(2)，计划可迭代」的代表，并与本架构对照。

- **出处与定位**：*Enhancing LLM-Based Agents via Global Planning and Hierarchical Execution*. arXiv:2504.16563 (2025), NCIIP 2025 **Best Paper**。代码见 GitHub（如 cjj826/GoalAct）。**持续更新的全局规划** + **分层技能执行**（搜索、编码、写作等）。
- **核心思想与协议**：与「先定死计划再执行」不同，全局计划随执行反馈**迭代更新**，兼顾高层引导与局部适应；高层技能（搜索/编码/写作）降低规划复杂度并提升跨任务适应性。
- **实验基准与主要结果**：LegalAgentBench 上相对基线平均提升 **12.22%**，达 SOTA。
- **与本架构的对应关系**：本架构无显式「持续更新的全局计划」；若需「路标式计划 + 随反馈更新 + 技能分层」，可参考 GoalAct 在上层增加可迭代计划与技能路由。选型时若强调「计划可迭代 + 多技能」，可优先参考 GoalAct。

### 9.7.10 Vote-Tree-Planner（表格项 #9）：树式+多计划选择与本架构对照

本节对 9.7 研究总表**第 9 项 Vote-Tree-Planner** 做专门研究性归纳，作为「树式 + 多计划选择」形态的代表，并与本架构对照。

- **出处与定位**：*Optimizing Execution Order in LLM-based Task Planning Pipeline via Voting*. arXiv:2502.09749 (2025)。在 Tree-Planner 的**动作树**上增加**投票**机制。
- **核心思想与协议**：在真正执行前，对关键路径或分支用多个 LLM 调用（或多次采样）做**评估与投票**，选出更可靠的执行顺序或分支，再沿该路径执行；在保持或提升成功率的同时，减少无效分支展开以控制 LLM 调用次数。对应 9.6 的**多计划选择**（Plan Selection）与树式结合。
- **与本架构的对应关系**：本架构为单轨迹决策；若需「多候选路径 + 投票选优」再执行，可参考 Vote-Tree-Planner 在 Tree-Planner 式扩展上增加投票层。选型时若强调「稳定性与成功率」，可参考 Vote-Tree-Planner。

### 9.7.11 HiPlan（表格项 #10）：轻量规划+反应执行（里程碑式）与本架构对照

本节对 9.7 研究总表**第 10 项 HiPlan** 做专门研究性归纳，作为「轻量规划+反应执行(2)，里程碑式」的代表，并与本架构对照。

- **出处与定位**：*Adaptive Hierarchical Planning with Milestone-based Guidance*. arXiv:2508.19076 (2025)。针对长时程任务中模型易迷失全局目标的问题，用**专家示范**离线构建**里程碑动作库**（milestone action guides）。
- **核心思想与协议**：执行时提供**自适应全局–局部引导**：高层给出当前阶段应达成的里程碑，局部给出逐步提示（step-wise hints），可根据当前观测动态选择与当前状态最匹配的里程碑与提示；规划侧以「里程碑 + 提示」注入，而非显式步骤列表。
- **与本架构的对应关系**：本架构无显式里程碑库；若需「里程碑路标 + 逐步提示」以减轻长链迷失，可参考 HiPlan 在上层增加里程碑检索与提示注入。选型时若强调「长时程 + 防迷失」，可参考 HiPlan。

### 9.7.12 ReAcTree（表格项 #11）：分层+控制流与本架构对照

本节对 9.7 研究总表**第 11 项 ReAcTree** 做专门研究性归纳，作为「分层(1) + 控制流(4)」形态的代表，并与本架构对照。

- **出处与定位**：*Hierarchical LLM Agent Trees with Control Flow for Long-Horizon Task Planning*. arXiv:2511.02424 (2025)。在 ReAct 式 Agent 之上引入**动态构造的 Agent 树**：Agent 节点负责推理与行动并可继续分解子目标，**控制流节点**负责序列（→）、回退（?）、并行（⇒）等执行策略（类行为树）。情节记忆（目标/子目标级示例）+ 工作记忆（环境观测）。
- **实验基准与主要结果**：WAH-NL 上 Qwen 2.5 72B 达 **61%** 目标成功率，约为纯 ReAct（31%）的近两倍；ALFRED 上也有稳定提升。
- **与本架构的对应关系**：本架构为单链 Agent 循环；若需「树式分解 + 控制流（序列/回退/并行）」，可参考 ReAcTree 在上层增加 Agent 树与控制流节点。选型时若强调「长时程 + 控制流可表达」，可优先参考 ReAcTree。

### 9.7.13 Beyond ReAct（表格项 #12）：显式 DAG 规划+训练增强与本架构对照

本节对 9.7 研究总表**第 12 项 Beyond ReAct** 做专门研究性归纳，作为「显式 DAG 规划+执行(1)，训练增强」的代表，并与本架构对照。

- **出处与定位**：arXiv:2511.10037 (2025)。以 **Planner 为中心**，用 **DAG 全局规划** 替代 ReAct 的逐步决策：先对复杂查询生成全局 DAG 计划（多步、多分支、多工具协同），再按 DAG 执行。
- **核心思想与协议**：摆脱 ReAct 的局部优化陷阱与工具协调不足；通过 **SFT + GRPO（Group Relative Policy Optimization）** 两阶段训练，提升模型的规划意识与工具选择准确性。
- **与本架构的对应关系**：本架构无显式 DAG 规划与 GRPO 训练；若需「DAG 计划 + 执行 + 训练增强」，可参考 Beyond ReAct 作为上层扩展或训练目标。选型时若强调「规划与工具选择可训练」，可参考 Beyond ReAct。

### 9.7.14 Reason-Plan-ReAct（RP-ReAct）（表格项 #13）：分层+上下文/存储分离与本架构对照

本节对 9.7 研究总表**第 13 项 Reason-Plan-ReAct (RP-ReAct)** 做专门研究性归纳，作为「分层(1) + 上下文/存储分离」的代表，并与本架构对照。

- **出处与定位**：*Reason-Plan-ReAct: A Reasoner-Planner Supervising a ReAct Executor for Complex Enterprise Tasks*. arXiv:2512.03560 (2025)。将**战略规划**与**执行**解耦：**RPA（Reasoner-Planner Agent）** 负责高层任务分解、计划生成与执行结果分析（维护信念状态、生成抽象子问题）；**PEA（Proxy-Execution Agent）** 将抽象子步骤转为具体工具调用，采用 ReAct 的 Thought→Action→Observation 循环。
- **核心思想与协议**：针对单 Agent 长链轨迹不稳定与上下文易溢出，引入**外部存储**管理大工具输出，按需加载而非全部塞入上下文。在 ToolQA 等企业多域任务上优于单 Agent，且在不同模型规模上更稳。
- **与本架构的对应关系**：本架构为单 Agent、上下文含全历史；若需「RPA+PEA 分层 + 大输出外存、按需加载」，可参考 RP-ReAct 在上层增加规划层与存储模块。选型时若强调「企业多域 + 上下文可控」，可参考 RP-ReAct。

### 9.7.15 Task-Decoupled Planning（TDP）（表格项 #14）：分层+作用域隔离与本架构对照

本节对 9.7 研究总表**第 14 项 Task-Decoupled Planning (TDP)** 做专门研究性归纳，作为「分层(1) + 作用域隔离」的代表，并与本架构对照。

- **出处与定位**：*Beyond Entangled Planning: Task-Decoupled Planning for Long-Horizon Agents*. arXiv:2601.07577 (2025)。解决「纠缠上下文」：传统做法在单一长链中同时维护多子任务历史，易导致错误传播与认知负荷过高。
- **核心思想与协议**：**Supervisor** 将复杂任务分解为带前驱约束的 **DAG 子目标**；**Planner** 与 **Executor** 仅在**当前活跃子任务**的作用域内推理与执行，不携带全任务历史。错误被限制在子任务内、可局部重规划或修正，而不影响整体 DAG。在 TravelPlanner、ScienceWorld、HotpotQA 上 token 消耗较基线减少约 **82%**，鲁棒性提升。
- **与本架构的对应关系**：本架构为单会话全历史；若需「DAG 子目标 + 按子任务作用域隔离上下文」以降 token、提鲁棒性，可参考 TDP 在上层增加 Supervisor 与作用域隔离。选型时若强调「token 大幅降低 + 错误隔离」，可优先参考 TDP。

### 9.8 配置命名说明

- 配置中的「Plan」若指**只读代理**（仅开放读与搜索类工具、不开放写与执行），与架构上的「规划器（Planner）」无直接对应，仅为代理配置的一种命名（例如「仅规划/探索、不落盘」的用法）。

---

## 10. 结论

本文以与实现语言无关的方式描述了「使 LLM 能安全写代码」的架构：**Agent 循环**（消息历史 + 流式 LLM 调用 + 工具执行 + 结果写回历史并继续）负责决策与执行的衔接；**Provider 抽象**负责多后端与 Tool Use 的统一；**edit / patch / write** 三种工具在**先读再改、唯一匹配、权限请求、diff、历史与 LSP** 的约束下完成对文件系统的修改。系统提示词约束模型按「读→改→查」与最小改动行事，且不自动提交。在**Agent 与 Planner** 的区分上，本架构采用 **ReAct 式**反应式 Agent（隐式规划、每步依赖环境反馈），不引入独立 Plan-and-Execute 规划器；Planner 的存在是为了在多步、结构清晰的任务上弥补 Agent 在成本、全局一致性与控制流可控性上的不足。两者可在同一系统中融合（如分层 Planner+Agent、轻量规划+反应执行、按需重规划或统一循环内的计划状态），当前文档以纯 Agent 闭环为基线，融合形态可在不改变核心消息与工具协议的前提下在上层扩展。该设计可在任意语言与运行时中实现，只需满足上述组件接口与数据流即可复现「能写代码」的核心行为。

---

## 附录 A：学术研究流程简述

本节简述如何围绕本架构或类似课题开展学术研究，供读者在复现、扩展或发表相关工作时参考。与实现语言无关，适用于计算机/软件工程/人机交互等方向。

### A.1 选题与问题界定

- **兴趣与可行性**：在感兴趣的方向上，选择在现有条件下可完成的题目。
- **文献摸底**：先读综述与顶会/顶刊近期论文，判断已有工作做到哪一步、存在哪些缺口或未解决问题。
- **收敛为研究问题**：将大方向收敛为可回答的**研究问题**（research questions），并写清问题陈述。

### A.2 文献综述

- **系统检索**：用关键词在 Google Scholar、顶会/期刊、预印本（如 arXiv）中检索，并顺藤摸瓜（参考文献与引用该文的文献）。
- **分类与脉络**：按主题/方法/时间整理，画出「谁在什么基础上做了什么」的脉络图。
- **找缺口**：明确本工作要填补的是方法不足、场景缺失、理论不清还是实证空白。

### A.3 研究设计与方法

- **研究类型**：实证（实验/调查/案例）、理论（建模/证明）、系统/工程（设计+实现+评估）等，须与问题匹配。
- **数据与材料**：需要什么数据、从哪获取、如何清洗；若做系统，需明确平台、接口与实验环境。
- **伦理与可复现**：涉及人、数据或代码时，遵守伦理审查与可复现性规范（如预注册、代码/数据开放）。

### A.4 执行与迭代

- **小步验证**：先做最小可行实验或原型，尽早验证核心假设。
- **记录与版本管理**：实验配置、代码、数据版本、想法与失败尝试均记录（如 lab notebook、Git、实验表）。
- **与导师/同行交流**：定期汇报进展、获取反馈，避免在错误方向上走远。

### A.5 分析与写作

- **分析**：按所选方法做统计分析、定性编码、理论推导或系统评估，并直接对应研究问题。
- **论文结构**：常见为引言（问题+贡献）→ 相关工作 → 方法 → 实验/案例 → 讨论 → 结论；不同领域可有差异。
- **叙事线**：强调「问题—现有不足—本工作做法—证据—含义」，保持逻辑一致。
- **反复修改**：多轮修改结构与表述，必要时请导师或同行做 critical reading。

### A.6 发表与反馈

- **选刊/选会**：按领域惯例选择期刊或会议，兼顾影响因子、接受率、审稿周期与读者群。
- **按规范投稿**：严格遵守格式、篇幅与匿名要求（双盲时注意去标识）。
- **审稿意见**：将审稿意见视为反馈，逐条回应；reject 也可作为改进下一稿的依据。

### A.7 与本架构的关系

- 本架构文档（第 2–9 节）本身是**系统/工程类**研究产出：以抽象组件、数据流与协议描述为主，不依赖具体实现语言。
- 围绕本架构可开展的学术工作包括：**复现与对比**（不同后端、不同工具集的实现与评估）、**扩展与融合**（如第 9.6、9.7 节的 Planner 融合形态）、**实证研究**（在真实任务上的成功率、延迟、成本与用户体验）、**理论分析**（规划能力、安全性、可审计性等）。选题时可结合 A.1–A.2 的文献综述确定缺口，再按 A.3–A.5 设计方法与写作。

---

## 参考文献（建议）

### Agent 与规划（学术综述与范式）

- **Huang et al. (2024)**. *Understanding the planning of LLM agents: A survey*. arXiv:2402.02716. 将 LLM-Agent 规划归纳为五类：Task Decomposition（含 decomposition-first 与 interleaved）、Plan Selection、External Module、Reflection and Refinement、Memory-augmented Planning.
- **Yao et al. (2022)**. *ReAct: Synergizing Reasoning and Acting in Language Models*. arXiv:2210.03629. ReAct 范式：推理与行动交替、基于环境反馈的闭环.
- **Wang et al. (2023)**. *Plan-and-Solve Prompting: Improving Zero-shot Chain-of-Thought Reasoning by Large Language Models*. arXiv:2305.04091. 「先制定计划再执行」的提示策略.
- **Shen et al. (2023)**. *HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Huggingface*. arXiv:2303.17580. 分解优先、再选模型与执行的 Plan-and-Execute 式系统.
- **Shinn et al. (2023)**. *Reflexion: Language Agents with Verbal Reinforcement Learning*. NeurIPS 2023. 基于轨迹评估与自我反思的纠错.
- **Masterman et al. (2024)**. *The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey*. arXiv:2404.11584. Agent 架构中的推理、规划与工具调用阶段划分.
- **Li et al. (2024)**. *A Review of Prominent Paradigms for LLM-Based Agents: Tool Use, Planning, and Feedback Learning*. arXiv:2406.05804 (CoLING 2025). 统一视角：工具使用（含 RAG）、规划与反馈学习三范式，以及 policy/evaluator 等 LMPR 角色与通用工作流.

### Agent 与 Planner 融合（近期实证研究）

- **ReAcTree**. *Hierarchical LLM Agent Trees with Control Flow for Long-Horizon Task Planning*. arXiv:2511.02424. Agent 节点 + 控制流节点（序列/回退/并行），情节记忆与工作记忆；WAH-NL 61% vs ReAct 31%.
- **GoalAct**. *Enhancing LLM-Based Agents via Global Planning and Hierarchical Execution*. arXiv:2504.16563 (NCIIP 2025 Best Paper). 持续更新的全局规划 + 分层技能执行；LegalAgentBench +12.22%.
- **Plan-and-Act**. *Improving Planning of Agents for Long-Horizon Tasks*. arXiv:2503.09572 (ICML 2025). Planner 模型 + Executor 模型，合成数据训练 Planner，动态重规划；WebArena-Lite 57.58%，WebVoyager 81.36%.
- **CoAct**. *A Global-Local Hierarchy for Autonomous Agent Collaboration*. arXiv:2406.13381. 全局规划 Agent + 局部执行 Agent，失败时重排轨迹；WebArena 长时程任务.
- **Plan-on-Graph (PoG)**. *Self-Correcting Adaptive Planning of Large Language Model on Knowledge Graphs*. arXiv:2410.23875. 子目标分解、路径探索与记忆更新、反思自校正.
- **Reason-Plan-ReAct (RP-ReAct)**. *Reason-Plan-ReAct: A Reasoner-Planner Supervising a ReAct Executor for Complex Enterprise Tasks*. arXiv:2512.03560. RPA（高层分解与结果分析）+ PEA（ReAct 工具执行），外部存储缓解上下文溢出；ToolQA.
- **Task-Decoupled Planning (TDP)**. *Beyond Entangled Planning: Task-Decoupled Planning for Long-Horizon Agents*. arXiv:2601.07577. Supervisor（DAG 子目标）+ Planner/Executor 作用域推理；token −82%，TravelPlanner/ScienceWorld/HotpotQA.
- **Tree-Planner**. *Efficient Close-loop Task Planning with Large Language Models*. arXiv:2310.08582. 计划采样、动作树构造、接地决策；token −92.2%.
- **Vote-Tree-Planner**. *Optimizing Execution Order in LLM-based Task Planning Pipeline via Voting*. arXiv:2502.09749. 在 Tree-Planner 基础上用投票优化执行顺序.
- **HiPlan**. *Adaptive Hierarchical Planning with Milestone-based Guidance*. arXiv:2508.19076. 里程碑动作库 + 自适应全局–局部引导（里程碑 + 逐步提示）.
- **AdaPlanner**. *Adaptive Planning from Feedback with Language Models*. NeurIPS 2023. 计划内/计划外自适应精炼、成功计划少样本复用；ALFWorld、MiniWoB++.
- **Beyond ReAct**. *Planner-centric Framework with DAG Planning*. arXiv:2511.10037. DAG 全局规划，SFT + GRPO 两阶段训练.
- **Plan-over-Graph**. *Plan-over-Graph* (或类似标题). arXiv:2502.14563. 子任务分解与抽象任务图，支持并行执行.

### 业界与教程（高价值参考）

- **LangChain**. *Plan-and-Execute Agents*. blog.langchain.com. Plan-and-Execute 的 planner + executor 两阶段设计、与 ReAct 的对比及适用场景.
- **James Li**. *ReAct vs Plan-and-Execute: A Practical Comparison of LLM Agent Patterns*. dev.to. 两种模式的实践对比与选型建议.
- **byaiteam**. *AI Agent Planning: ReAct vs Plan and Execute for Reliability*. 从可靠性角度对比 ReAct 与 Plan-and-Execute，含控制流与间接提示注入等考量.
- **krasserm**. *Separation of planning concerns in ReAct-style LLM agents*. krasserm.github.io. ReAct 式 Agent 中规划关注点的模块化分离思路.

### 实现与协议

- 各 LLM API 文档：OpenAI Function Calling, Anthropic Tool Use, 等.
- Language Server Protocol (LSP) 规范.
- Unified Diff 格式 (e.g., POSIX diff).
