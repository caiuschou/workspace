# 自动测试生成 Agent - 技术方向深度分析

> 基于第一性原理的思考，从5个方向中筛选出最可行的单一方向。

## 终极目标：用户层面保障

**测试的最终目的**：在**用户层面**提供保障 —— 用户能顺利完成预期任务，产品在真实使用场景中表现正确。

> 单元测试通过 ≠ 用户能完成任务

| 测试层级 | 验证范围 | 与用户保障的关联 | 生成难度 |
|----------|----------|------------------|----------|
| 单元测试 | 单个函数/类 | 间接：防内部 bug | 较易 |
| 集成测试 | 模块间协作 | 中等：更接近真实流程 | 较难 |
| E2E 测试 | 完整用户流程 | **直接：用户能完成任务** | 最难 |

**演进路径**：从现在的单元测试生成（技术可行、快速验证）→ 未来的用户层面保障（E2E、用户流程测试）。

---

## 为什么选择测试生成（现在 → 未来的切入点）

### 第一性原理验证

| 维度 | 分析 |
|------|------|
| **验证成本** | 极低 - 测试天然可执行，跑一下就知道对不对 |
| **Feedback Loop** | 极短 - 秒级得到pass/fail结果 |
| **风险** | 低 - 不触及生产环境，最坏情况只是测试没用 |
| **LLM能力匹配** | 高 - 代码生成是LLM最强的能力之一 |
| **需求普遍性** | 极高 - 几乎所有项目都缺测试 |
| **付费意愿** | 中高 - CI/CD预算稳定，测试覆盖率是硬指标 |

### 与其他方向对比

- **代码迁移**：验证成本高（迁移后系统是否正常需要大量测试），信任建立难
- **DevOps运维**：生产环境自动执行风险极高
- **安全修复**：责任重大，出错代价高
- **多模型路由**：技术壁垒低，大厂容易复制

**测试生成是唯一一个"错了也不会炸"的方向**。

---

## 阶段一：现在 - 单元测试生成

### 核心价值主张

> 把"写测试"这件开发者最不想做但必须做的事，变成自动化流程。（当前聚焦单元测试，作为通往用户层面保障的基石）

### 目标用户画像

1. **Startup快速迭代团队** - 测试覆盖率低，技术债累积
2. **遗留项目维护者** - 老代码没测试，改动就心慌
3. **Solo开发者/小团队** - 知道要写测试但没时间
4. **CI/CD强制要求的团队** - coverage门槛是硬性要求

### 解决的具体痛点

- 写测试枯燥、耗时
- 不知道该测什么边界条件
- 测试和代码不同步
- 新人不熟悉项目，不知道怎么写测试

---

## 技术架构设计

### Agent Graph 拓扑

```
[Code Change Detection]
         │
         ▼
[Code Understanding Node]
    - 解析函数签名
    - 提取依赖关系
    - 识别边界条件
         │
         ▼
[Test Generation Node]
    - 生成单元测试
    - 生成边界测试
    - 生成异常测试
         │
         ▼
[Test Execution Node]
    - 沙箱执行测试
    - 收集覆盖率
    - 捕获错误信息
         │
         ▼
    ┌────┴────┐
    │ Pass?   │
    └────┬────┘
    Yes  │  No
     │   │   │
     ▼   │   ▼
[Report] │ [Self-Repair Node]
         │   - 分析失败原因
         │   - 修正测试代码
         │   - 重新生成
         │       │
         └───────┘ (loop max 3次)
```

### 关键技术组件

#### 1. 代码理解层
- **Tree-sitter** - 多语言AST解析
- **函数签名提取** - 输入输出类型分析
- **依赖图构建** - mock对象识别

#### 2. 测试生成层
- **Prompt模板** - 针对不同语言/框架优化
- **边界条件推理** - 空值、边界值、异常值
- **Mock生成** - 外部依赖的mock策略

#### 3. 执行验证层
- **沙箱环境** - Docker容器隔离执行
- **覆盖率收集** - 行覆盖、分支覆盖
- **错误分类** - 语法错误 vs 断言失败 vs 运行时错误

#### 4. 自修复层
- **错误分析** - stack trace解析
- **修复策略** - 针对不同错误类型的修复prompt
- **循环控制** - 最大重试次数，避免无限循环

---

## 支持的语言/框架（MVP）

| 语言 | 测试框架 | 优先级 |
|------|----------|--------|
| Java | JUnit | P0 |
| Python | pytest | P0 |
| TypeScript/JavaScript | Jest/Vitest | P0 |
| Rust | cargo test | P1 |
| Go | go test | P1 |

**MVP策略**：先做 Java + JUnit，验证核心流程后扩展。

**验证目标**：Apache Dubbo
- 42k+ stars，流行 RPC 框架
- 官方有 [Test Coverage Guide](https://dubbo.apache.org/en/docs/contribution-guidelines/contributor/test-coverage-guide_dev/)，增量要求 ≥60%
- CodeCov 追踪，可量化效果
- 建议从 `dubbo-common` 等基础模块切入

---

## 生产级要求

### 可靠性保障

- [ ] 测试执行超时控制（单个测试最大30秒）
- [ ] 资源隔离（CPU/内存限制）
- [ ] 并发控制（避免资源竞争）
- [ ] 幂等性（重复执行结果一致）

### 可观测性

- [ ] 每次生成的详细日志
- [ ] 成功率/失败率统计
- [ ] LLM调用成本追踪
- [ ] 覆盖率变化趋势

### 集成能力

- [ ] GitHub Actions / GitLab CI
- [ ] PR触发（代码变更时自动生成）
- [ ] 覆盖率报告输出
- [ ] 注释回复PR

---

## MVP定义

### 第一版目标（2周）

**输入**：一个 Java 类/方法文件
**输出**：对应的 JUnit 测试文件

**核心流程**：
1. 读取函数代码
2. 调用 LLM 生成 JUnit 测试
3. 执行测试
4. 如果失败，分析错误，重新生成（最多3次）
5. 输出最终测试文件 + 执行报告

**不包含**：
- CI集成
- 多文件/多语言
- Web UI
- 覆盖率优化

### 成功标准

- 对于简单方法（纯方法，无外部依赖），生成的测试通过率 > 80%
- 对于中等复杂度方法（有参数验证），生成的测试通过率 > 60%
- 平均生成时间 < 30秒

---

## Prompt 模板设计

### 代码理解 Prompt

```
分析以下 Java 方法，提取：
1. 方法名称和用途
2. 参数类型和约束
3. 返回值类型
4. 可能的边界条件
5. 可能抛出的异常
6. 外部依赖（需要 mock 的对象）

```java
{code}
```

输出 JSON 格式：
{
  "method_name": "",
  "purpose": "",
  "parameters": [...],
  "return_type": "",
  "edge_cases": [...],
  "exceptions": [...],
  "dependencies": [...]
}
```

### 测试生成 Prompt

```
基于以下方法分析，生成 JUnit 5 测试用例。

方法代码：
```java
{code}
```

分析结果：
{analysis_json}

要求：
1. 覆盖正常路径
2. 覆盖所有边界条件
3. 覆盖异常情况
4. 使用 Mockito 等 mock 依赖（如适用）
5. 添加清晰的测试方法命名（test_<功能>_<场景>）
6. 添加 @DisplayName 或 docstring 说明测试意图

只输出 JUnit 5 测试代码，不要解释。
```

### 自修复 Prompt

```
测试执行失败，请修复。

原始测试代码：
```java
{test_code}
```

错误信息：
```
{error_message}
```

被测试的方法代码：
```java
{original_code}
```

分析错误原因，输出修复后的完整 JUnit 测试代码。
只输出代码，不要解释。
```

---

## 风险与缓解

| 风险 | 影响 | 缓解措施 |
|------|------|----------|
| LLM生成的测试质量不稳定 | 用户信任度下降 | 多轮自修复 + 明确告知"需要人工审核" |
| 复杂依赖难以mock | 生成失败率高 | MVP先专注纯函数，逐步扩展 |
| 执行环境差异 | 本地pass远程fail | 标准化Docker执行环境 |
| LLM成本控制 | 烧钱 | 设置token上限，优化prompt长度 |

---

## 变现路径

### 阶段一：开源核心
- GitHub开源基础版
- 支持CLI使用
- 目标：积累star，验证需求

### 阶段二：付费托管
- 提供云端执行环境
- GitHub/GitLab集成
- 定价：$19-49/月 per repo

### 阶段三：企业版
- 私有部署
- 批量repo支持
- 定制语言/框架
- 定价：$199-499/月

---

## 阶段二：未来 - 用户层面保障

单元测试是**必要条件**（防内部 bug），但非**充分条件**（不直接保证用户能完成任务）。演进到用户层面保障时，需要：

### 未来方向

| 方向 | 描述 | 技术要点 |
|------|------|----------|
| **用户故事 → E2E 测试** | 从用户故事/验收标准生成端到端测试 | Playwright、Cypress 等，模拟真实用户操作 |
| **关键路径覆盖** | 识别核心业务流程，优先保障 | 调用链分析、用户行为埋点反哺 |
| **可观测性 + 测试** | 用真实使用数据反哺「应该测什么」 | 生产环境埋点 → 高频路径 → 生成对应测试 |

### 与现在的关系

```
现在（单元测试生成）          未来（用户层面保障）
        │                              │
        │   积累：                      │
        │   - 代码理解能力              │
        │   - 测试生成流程              │
        │   - 自修复机制                │
        │   - 执行沙箱                  │
        └──────────────────────────────┘
                    │
                    ▼
            集成测试生成（中间态）
                    │
                    ▼
            E2E / 用户流程测试生成
```

**当前选择单元测试的原因**：技术可行、验证成本低、feedback loop 短，是通往用户层面保障的**可落地的第一步**。

---

## 下一步行动

### 现在（MVP）
- [ ] 搭建MVP项目骨架
- [ ] 实现代码解析模块（Tree-sitter for Java）
- [ ] 设计并测试Prompt模板
- [ ] 实现 JUnit 执行沙箱
- [ ] 端到端验证：在 Dubbo 的 dubbo-common 中选取方法，生成测试并对比 CodeCov

### 未来（用户层面保障）
- [ ] 集成测试生成（跨模块、API 层）
- [ ] 用户故事 → E2E 测试（Playwright/Cypress）
- [ ] 关键路径识别与优先保障
- [ ] 可观测性数据反哺测试生成
